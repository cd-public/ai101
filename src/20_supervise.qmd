---
title: (Un)supervision
---

## Different Approaches to AI

If we want a computer to behave like a human, we need somehow to model inside a computer our way of thinking. Consequently, we need to try to understand what makes a human being intelligent.

> To be able to program intelligence into a machine, we need to understand how our own processes of making decisions work. If you do a little self-introspection, you will realize that there are some processes that happen subconsciously ‚Äì eg. we can distinguish a cat from a dog without thinking about it - while some others involve reasoning.

## Different Approaches to AI

Top-down Approach (Symbolic Reasoning) | Bottom-up Approach (Neural Networks)
---------------------------------------|-------------------------------------
A top-down approach models the way a person reasons to solve a problem. It involves extracting **knowledge** from a human being, and representing it in a computer-readable form. We also need to develop a way to model **reasoning** inside a computer. | A bottom-up approach models the structure of a human brain, consisting of a huge number of simple units called **neurons**. We can train a network of neurons to solve useful problems by providing **training data**.

## Different Approaches to AI

There are also some other possible approaches to intelligence:

* An **Emergent**, **Synergetic** or **multi-agent approach** are based on the fact that complex intelligent behaviour can be obtained by an interaction of a large number of simple agents. According to [evolutionary cybernetics](https://en.wikipedia.org/wiki/Global_brain#Evolutionary_cybernetics), intelligence can *emerge* from more simple, reactive behaviour in the process of *metasystem transition*.
* An **Evolutionary approach**, or **genetic algorithm** is an optimization process based on the principles of evolution.

## Aside: CSAIL

![](https://archive.org/services/img/fleshmachines00rodn/full/pct:200/0/default.jpg)

## The Top-Down Approach

- In a **top-down approach**, we try to model our reasoning.  
- Because we can follow our thoughts when we reason, we can try to formalize this process and program it inside the computer. 
- This is called **symbolic reasoning**, one example is grade-school *algebra*

```{.py}
x = 2
y = 2 * x
```

## The Top-Down Approach

- People tend to have some rules in their head that guide their decision making processes. 
    - At least according to them (go observe rush hour drivers for example)
- For example, when a doctor is diagnosing a patient, they may realize that a person has a fever, and thus there might be some inflammation going on inside the body. 
- By applying a large set of rules to a specific problem a doctor may be able to come up with the final diagnosis.

## The Top-Down Approach

- This approach relies heavily on **knowledge representation** and **reasoning**. 
- Extracting knowledge from a human expert might be the most difficult part, because a doctor in many cases would not know exactly why they are coming up with a particular diagnosis. 
- Sometimes the solution just comes up in their head without explicit thinking. 
- Some tasks, such as determining the age of a person from a photograph, cannot be reduced to manipulating knowledge.

## Bottom-Up Approach

- Alternately, we can try to model the simplest *atomic* elements inside our brain ‚Äì a neuron. 
    - Obviously there are simpler elements (e.g. chemicals) but they take on different meaning in when not considered in isolation.
    - Therefore they are not *atomic*, or indivisible units of "brain".
    
## Bottom-Up Approach
    
- We can construct a so-called **artificial neural network** inside a computer, and then try to teach it to solve problems by giving it examples. 
    - Allegedly this process is similar to how a newborn child learns about their surroundings by making observations.
- Simply program a computer to pretend to be however we think brains work.
    - Of note, in some sense it doesn't matter if we model the brain correctly or not.

## A Brief History of AI

- Artificial Intelligence was started as a field in the middle of the twentieth century. 
- Initially, symbolic reasoning was a prevalent approach, and it led to a number of important successes, such as expert systems ‚Äì computer programs that were able to act as an expert in some limited problem domains. 
- However, it soon became clear that such approach does not scale well. 

## Problems arise

- Extracting the knowledge from an expert, representing it in a computer, and keeping that knowledgebase accurate turns out to be a very complex task, and too expensive to be practical in many cases. 
- This led to so-called [AI Winter](https://en.wikipedia.org/wiki/AI_winter) in the 1970s.

## 

<img style="filter:invert(.9)" alt="Brief History of AI" src="images/history-of-ai.png" width="70%"/>

*Image by [Dmitry Soshnikov](http://soshnikov.com)*

## Computers Emerge

- As time passed...
    - Computing resources became cheaper, and 
    - More data has become available...
- So neural network approaches started demonstrating great performance in competing with human beings in many areas, such as computer vision or speech understanding. 

*In the last decade, the term Artificial Intelligence has been mostly used as a synonym for Neural Networks, because most of the AI successes that we hear about are based on them.*

## Chess

We can observe how the approaches changed, for example, in creating a chess playing computer program:

- Early chess programs were based on search ‚Äì a program explicitly tried to estimate possible moves of an opponent for a given number of next moves, and selected an optimal move based on the optimal position that can be achieved in a few moves. It led to the development of the so-called [alpha-beta pruning](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning) search algorithm.

## Chess

We can observe how the approaches changed, for example, in creating a chess playing computer program:

- Search strategies work well toward the end of the game, where the search space is limited by a small number of possible moves. However, at the beginning of the game, the search space is huge, and the algorithm can be improved by learning from existing matches between human players. 
- Subsequent experiments employed so-called [case-based reasoning](https://en.wikipedia.org/wiki/Case-based_reasoning), where the program looked for cases in the knowledge base very similar to the current position in the game.

## Chess

We can observe how the approaches changed, for example, in creating a chess playing computer program:

-  Modern programs that win over human players are based on neural networks and [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning), where the programs learn to play solely by playing a long time against themselves and learning from their own mistakes ‚Äì much like human beings do when learning to play chess. 
- However, a computer program can play many more games in much less time, and thus can learn much faster.

## Chatbots

Similarly, we can see how the approach towards creating ‚Äútalking programs‚Äù (that might pass the Turing test) changed:

- Early programs of this kind such as [Eliza](https://en.wikipedia.org/wiki/ELIZA), were based on very simple grammatical rules and the re-formulation of the input sentence into a question.

## Chatbots

Similarly, we can see how the approach towards creating ‚Äútalking programs‚Äù (that might pass the Turing test) changed:

- Modern assistants, such as Cortana, Siri or Google Assistant are all hybrid systems that use Neural networks to convert speech into text and recognize our intent, and then employ some reasoning or explicit algorithms to perform required actions.

## Chatbots

Similarly, we can see how the approach towards creating ‚Äútalking programs‚Äù (that might pass the Turing test) changed:

- Depending on who you ask, this is where we are now.
    - If you ask me, "no".
        - My job is to know how to answer this question.
    - If you ask Sam Altman, "yes".
        - Sam Altman's job is convincingly say "yes" when asked this question.

## 

<img alt="the Turing test's evolution" src="images/turing-test-evol.png" width="70%"/>

*Image by Dmitry Soshnikov, [photo](https://unsplash.com/photos/r8LmVbUKgns) by [Marina Abrosimova](https://unsplash.com/@abrosimova_marina_foto), Unsplash*

## Recent AI Research

- The huge recent growth in neural network research started around 2010, when two things happened.
    - More compute.
    - More data.

## More compute

- A graduate students at UToronto affiliated with Prof. Hinton decided to use a GPU (graphics processing unit) instead of CPU (central processing unit) for *image classification*.
- This is the first *documented* usage of a graphics-intended compute device for general purpose - or perhaps general AI - usage - of which I am aware.
    - This is not super well-documented and only something I became aware of "through the grapevine".
    - As an architecture researcher (e.g. I study GPU vs. CPU) I tend to think it's a big deal.
- Read more [here](https://arstechnica.com/ai/2024/11/how-a-stubborn-computer-scientist-accidentally-launched-the-deep-learning-boom/) perhaps?

## More data

:::: {.columns}

::: {.column width="70%"}

- Genius ÊùéÈ£ûÈ£û (Fei-Fei Li, my üêê) said (paraphrasing) "why don't we just label as many images as possible and see what happens".
- A huge collection of images called [ImageNet](https://en.wikipedia.org/wiki/ImageNet), which contains around 14 million annotated images, and led to the [ImageNet Large Scale Visual Recognition Challenge](https://image-net.org/challenges/LSVRC/).

:::

::: {.column width="30%"}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Fei-Fei_Li_at_AI_for_Good_2017.jpg/250px-Fei-Fei_Li_at_AI_for_Good_2017.jpg)

:::

::::


## The world changed quickly

![](images/ilsvrc.gif)

- 2012 AlexNet is from the Toronto team and the "quantum leap" on here.
- In 2015, ResNet architecture from Microsoft Research [achieved human-level accuracy](https://doi.org/10.1109/ICCV.2015.123).
    - Universities solve the problem.
    - Trillion dollar companies take the credit.
    - Ask me about this *outside* of class.

## Going forward

- Since then, Neural Networks demonstrated very successful behaviour in many tasks:

Year | Human Parity (nominally) achieved
-----|--------
2015 | [Image Classification](https://doi.org/10.1109/ICCV.2015.123)
2016 | [Conversational Speech Recognition](https://arxiv.org/abs/1610.05256)
2018 | [Automatic Machine Translation](https://arxiv.org/abs/1803.05567) (Chinese-to-English)
2020 | [Image Captioning](https://arxiv.org/abs/2009.13682)

# Supervision

## Motivating Questions

1. What is difference between supervised and unsupervised learning?
2. What are supervised and unsupervised learning used for?
3. What are some challenges with supervised and unsupervised learning?

## Supervised Learning

* Uses *labeled* data.
* Maps inputs to outputs.
* Goal: Predict outcomes.
* Examples: Classification, regression.

## Unsupervised Learning

* Uses *unlabeled* data.
* Finds hidden patterns.
* Goal: Discover structure.
* Examples: Clustering, dimensionality reduction.

## The Label Divide

* Supervised: Labeled data.
* Unsupervised: Unlabeled data.
* Label presence is key.

<center><a title="Balkiss.hamad, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Supervised_and_unsupervised_learning.png"><img alt="Difference between supervised and unsupervised learning" src="https://upload.wikimedia.org/wikipedia/commons/4/4d/Supervised_and_unsupervised_learning.png?20241123155435"></a></center>



## Uses of Unsupervised


* Clustering: Grouping similar data.
  * E.g. discover Yemenia novel coffee varietal
  * [Learn more](https://www.youtube.com/watch?v=-oiHm0wlhfM)
    * 18 min + can't watch on stream.
* Anomaly detection: Finding outliers.
* Dimensionality reduction: Simplifying data.
* Association rule learning: Finding relationships.



## Clustering in Action

* Customer/market segmentation.
  * Why EU entered "esports winter" 18+ months before US?
* Document clustering.
<center style="background-color:white;border-radius:50%"><a title="Original:  hellisp‚ÄÇVector:  Wgabrie, Public domain, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Cluster-2.svg"><img  alt="fotoƒüraf galerisi video resim haber i√ßerikleri y√ºkleyebilir" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Cluster-2.svg/512px-Cluster-2.svg.png?20100210061956"></a></cennter>



## Anomaly Detection

* Detect fraudulent bank transactions.
  * E.g. if I purchased an espresso in Salem, MA
* Detect anomalous (computer) network behavior.
  * My colleague at MS does this
* Predict imminent equipment failure in e.g. manufacturing sector.
  * Do you do this in 596?

# Challenges

## Challenges 

- Evaluation
  * No clear "right" answer.
    * And sometimes there's *someday* a right answer, but you have to make the model now.
    * Sometimes the "right" answer is determined by someone in power.
  * Subjective evaluation metrics.
    * Non-falsifiable by construction.
  * Validating discovered patterns.



## Challenges

- Interpretation
  * Understanding discovered structures.
    * Think in e.g. practice midterm - did you "discover" colleges vs. universities.
    * What about "big school" and "small school"
  * Meaningful insights can be hard.
    * Did you find "Colleges that Change Lives"
  * Requires or at least benefits from domain expertise.

## Challenges

- Computational Cost
  * Large datasets are common.
  * Algorithms can be complex.
  * Scalability is a concern.
- [Computational costs become operating costs](https://console.cloud.google.com/billing/013EFE-0A54F4-1CB6BE/reports/cost-breakdown;timeRange=CUSTOM_RANGE;from=2024-06-01;to=2025-02-28?organizationId=692811010335&project=dataproc-ce-project)

## Takeaways

* Supervised: Labeled data &rArr; predict label.
* Unsupervised: Unlabeled data &rArr; discover patterns.
  * Valuable, but challenging.
  * Value is correlated with challenge given scarcity.
  
# Supervised vs Unsupervised vs Reinforcement Learning

Machine learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention. The three primary types of ML are:

* **Supervised Learning:** Learning from labeled data.
* **Unsupervised Learning:** Discovering patterns in unlabeled data.
* **Reinforcement Learning:** Learning through interactions with an environment.

---

### 1. Supervised Learning
The model is trained on a labeled dataset, meaning each input has a corresponding output.
* **Labeled Data:** Training data has predefined labels.
* **Types of Problems:** Used for classification (e.g., spam detection) and regression (e.g., predicting house prices).
* **Algorithms:** Linear Regression, Logistic Regression, SVM, Decision Trees, Neural Networks.

### 2. Unsupervised Learning
The model identifies patterns, clusters, or associations independently without predefined labels.
* **Unlabeled Data:** No predefined outputs.
* **Types of Problems:** Used for clustering (e.g., customer segmentation) and association (e.g., market basket analysis).
* **Algorithms:** K-Means, Hierarchical Clustering, PCA, Autoencoders.

### 3. Reinforcement Learning (RL)
Involves an agent that interacts with an environment, learning through rewards and penalties to maximize long-term success.
* **Interaction-Based Learning:** The agent learns by taking actions and receiving feedback.
* **No Labeled Data:** Learns from trial and error.
* **Algorithms:** Q-learning, SARSA, Deep Q-Networks (DQN).

---

### Comparison Table: Supervised vs Unsupervised vs Reinforcement Learning

| Criteria | Supervised Learning | Unsupervised Learning | Reinforcement Learning |
| :--- | :--- | :--- | :--- |
| **Definition** | Learns from labeled data | Identifies patterns in unlabeled data | Learns through interaction with environment |
| **Type of Data** | Labeled data | Unlabeled data | No predefined data; learn from environment |
| **Type of Problems** | Classification, Regression | Clustering, Association | Sequential decision-making |
| **Supervision** | Requires external supervision | No supervision | No supervision; learns from feedback |
| **Algorithms** | SVM, Decision Trees, Neural Networks | K-Means, PCA, Autoencoders | Q-learning, DQN, SARSA |
| **Goal** | Predict outcomes accurately | Discover hidden patterns | Optimize actions for maximum rewards |
| **Applications** | Medical diagnosis, fraud detection | Customer segmentation, anomaly detection | Self-driving cars, robotics, gaming |

---

### Real-World Applications

| Machine Learning Type | Domain | Examples |
| :--- | :--- | :--- |
| **Supervised Learning** | Healthcare | Disease diagnosis (e.g., cancer detection) |
| **Supervised Learning** | Finance | Loan approval, credit risk assessment |
| **Supervised Learning** | NLP | Sentiment analysis, text classification |
| **Unsupervised Learning** | E-commerce | Product recommendation, customer segmentation |
| **Unsupervised Learning** | Cybersecurity | Fraud detection, intrusion detection |
| **Unsupervised Learning** | Biology | Gene classification, dimensionality reduction |
| **Reinforcement Learning** | Autonomous Driving | Self-driving cars learning optimal behavior |
| **Reinforcement Learning** | Robotics | Training robots for automated assembly tasks |
| **Reinforcement Learning** | Gaming | AI-driven strategy games like AlphaGo |

---

### Choosing the Right Approach
* **Supervised Learning:** Use when labeled data is available for prediction tasks.
* **Unsupervised Learning:** Use when exploring data structures without predefined labels.
* **Reinforcement Learning:** Use when decision-making is required in a dynamic environment.

https://microsoft.github.io/ML-For-Beginners/?id=content#/4-Classification/1-Introduction/README

https://microsoft.github.io/ML-For-Beginners/?id=content#/5-Clustering/README

https://microsoft.github.io/ML-For-Beginners/?id=content#/8-Reinforcement/README